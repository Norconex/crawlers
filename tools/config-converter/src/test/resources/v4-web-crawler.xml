<crawlSession id="Collector/CrawlSession Config">
  <workDir>/workdir</workDir>
  <eventListeners>
    <listener
        class="com.norconex.crawler.web.crawler.event.impl.URLStatusCrawlerEventListener">
      <statusCodes>404</statusCodes>
      <outputDir>/tmp/path</outputDir>
      <fileNamePrefix>broken-links</fileNamePrefix>
    </listener>
  </eventListeners>
  <crawlerDefaults>
    <start
        async="true"
        includeSubdomains="true"
        stayOnDomain="true"
        stayOnPort="true"
        stayOnProtocol="true">
      <ref>http://www.example.com</ref>
      <ref>http://www.sample.com</ref>
      <refsFile>/local/path/to/a/file/full/of/urls.txt</refsFile>
      <sitemap>http://www.somewhere.com/sitemap.xml</sitemap>
      <provider class="com.norconex.crawler.web.crawler.MockStartURLsProvider"/>
    </start>
    <keepReferencedLinks>INSCOPE, OUTSCOPE, MAXDEPTH</keepReferencedLinks>
    <maxDepth>99</maxDepth>
    <numThreads>1</numThreads>
    <maxDocuments>111</maxDocuments>
    <orphansStrategy>PROCESS</orphansStrategy>
    <metadataFetchSupport>OPTIONAL</metadataFetchSupport>
    <documentFetchSupport>DISABLED</documentFetchSupport>
    <eventListeners>
      <listener class="URLStatusCrawlerEventListener">
        <statusCodes>200</statusCodes>
        <outputDir>/tmp/okpath</outputDir>
        <fileNamePrefix>good-pages</fileNamePrefix>
      </listener>
    </eventListeners>
    <urlNormalizer
        class="com.norconex.crawler.web.url.impl.GenericURLNormalizer">
      <normalizations>
        lowerCaseSchemeHost, upperCaseEscapeSequence, 
        decodeUnreservedCharacters, removeDefaultPort
      </normalizations>
      <replacements>
        <replace>
          <match>A</match>
          <replacement>B</replacement>
        </replace>
        <replace>
          <match>C</match>
        </replace>
      </replacements>
    </urlNormalizer>
    <delay
        class="com.norconex.crawler.web.delay.impl.GenericDelayResolver"
        default="1s"
        ignoreRobotsCrawlDelay="true"
        scope="crawler">
      <schedule
          dayOfMonth="from 1 to 10"
          dayOfWeek="from Monday to Friday"
          time="from 8:00 to 16:30">
        10s
      </schedule>
      <schedule
          dayOfMonth="from 11 to 28"
          dayOfWeek="from Saturday to Sunday"
          time="from 6:00 to 7:30">
        20000
      </schedule>
    </delay>
    <dataStoreEngine
        class="com.norconex.crawler.core.store.impl.mvstore.MVStoreDataStoreEngine">
      <pageSplitSize>1000</pageSplitSize>
      <compress>1</compress>
      <cacheConcurrency>0</cacheConcurrency>
      <cacheSize>1000</cacheSize>
      <autoCompactFillRate>100</autoCompactFillRate>
      <autoCommitBufferSize>100</autoCommitBufferSize>
      <autoCommitDelay>100</autoCommitDelay>
    </dataStoreEngine>
    <fetchers maxRetries="5" retryDelay="10 seconds">
      <fetcher class="com.norconex.crawler.web.fetch.impl.GenericHttpFetcher">
        <userAgent>Here we crawl!</userAgent>
        <validStatusCodes>200,201</validStatusCodes>
        <notFoundStatusCodes>404,401</notFoundStatusCodes>
        <forceContentTypeDetection>true</forceContentTypeDetection>
        <forceCharsetDetection>true</forceCharsetDetection>
        <headersPrefix>myprefix</headersPrefix>
        <detectContentType>true</detectContentType>
        <detectCharset>true</detectCharset>
        <redirectURLProvider fallbackCharset="UTF-8"/>
        <cookieSpec>ignore</cookieSpec>
        <authentication>
          <formParams>
            <param name="param1">value1</param>
            <param name="param2">value2</param>
          </formParams>
          <method>digest</method>
          <credentials>
            <username>user</username>
            <password>pass</password>
            <passwordKey>
              <value>/path/to/my.key</value>
              <source>file</source>
            </passwordKey>
          </credentials>
          <formUsernameField>userfield</formUsernameField>
          <formPasswordField>pwdfield</formPasswordField>
          <formSelector>#myform</formSelector>
          <formCharset>UTF-8</formCharset>
          <url>authURL</url>
          <host>
            <name>host</name>
            <port>9</port>
          </host>
          <realm>authRealm</realm>
          <workstation>authWorkstation</workstation>
          <domain>authDomain</domain>
          <preemptive>true</preemptive>
        </authentication>
        <connectionTimeout>1</connectionTimeout>
        <socketTimeout>2 minutes</socketTimeout>
        <connectionRequestTimeout>3 min 30s</connectionRequestTimeout>
        <connectionCharset>US-ASCII</connectionCharset>
        <expectContinueEnabled>true</expectContinueEnabled>
        <maxRedirects>4</maxRedirects>
        <localAddress>address</localAddress>
        <maxConnections>5</maxConnections>
        <maxConnectionsPerRoute>6</maxConnectionsPerRoute>
        <maxConnectionIdleTime>7</maxConnectionIdleTime>
        <maxConnectionInactiveTime>8</maxConnectionInactiveTime>
        <trustAllSSLCertificates>true</trustAllSSLCertificates>
        <sniDisabled>true</sniDisabled>
        <sslProtocols>item1,item2</sslProtocols>
        <proxySettings>
          <host>
            <name>host</name>
            <port>9</port>
          </host>
          <realm>realm</realm>
          <scheme>scheme</scheme>
          <credentials>
            <username>username</username>
            <password>pwd</password>
            <passwordKey>
              <value>/path/to/my.key</value>
              <source>file</source>
            </passwordKey>
          </credentials>
        </proxySettings>
        <ifModifiedSinceDisabled>true</ifModifiedSinceDisabled>
        <headers>
          <header name="head1"></header>
          <header name="head2">value2</header>
        </headers>
      </fetcher>
    </fetchers>
    <referenceFilters>
      <filter
          class="com.norconex.crawler.core.filter.impl.ExtensionReferenceFilter"
          ignoreCase="false"
          onMatch="exclude">
        xml,pdf,doc
      </filter>
      <filter class=".GenericReferenceFilter" onMatch="exclude">
        <valueMatcher ignoreCase="false" method="regex">.*example.com.*</valueMatcher>
      </filter>
      <filter
          class="com.norconex.crawler.web.filter.impl.SegmentCountURLFilter"
          count="5"
          duplicate="false"
          onMatch="exclude">
        <separator>/</separator>
      </filter>
    </referenceFilters>
    <robotsTxt
        class="com.norconex.crawler.web.robot.impl.StandardRobotsTxtProvider"/>
    <sitemapResolver
        class="com.norconex.crawler.web.sitemap.impl.GenericSitemapResolver"
        lenient="true">
    </sitemapResolver>
    <sitemapLocator
        class="com.norconex.crawler.web.sitemap.impl.GenericSitemapLocator">
      <paths>
        <path>/path1/</path>
        <path>/path2/</path>
      </paths>
    </sitemapLocator>
    <recrawlableResolver
        class="com.norconex.crawler.web.recrawl.impl.GenericRecrawlableResolver"
        sitemapSupport="last">
      <minFrequency applyTo="reference" value="always">
        <matcher ignoreCase="true" method="regex">.*\.pdf</matcher>
      </minFrequency>
      <minFrequency applyTo="contentType" value="3000">
        <matcher ignoreCase="false" method="regex">text/html</matcher>
      </minFrequency>
    </recrawlableResolver>
    <metadataFilters>
      <filter
          class="com.norconex.crawler.core.filter.impl.ExtensionReferenceFilter"
          ignoreCase="false"
          onMatch="exclude">
        xml,pdf,doc
      </filter>
      <filter class=".GenericReferenceFilter" onMatch="exclude">
        <valueMatcher ignoreCase="true" method="regex">.*example.com.*</valueMatcher>
      </filter>
      <filter class=".GenericMetadataFilter" onMatch="include">
        <valueMatcher ignoreCase="true" method="regex">Blah.*</valueMatcher>
        <fieldMatcher>title</fieldMatcher>
      </filter>
      <filter
          class="com.norconex.crawler.web.filter.impl.SegmentCountURLFilter"
          count="5"
          duplicate="false"
          onMatch="exclude">
        <separator>/</separator>
      </filter>
    </metadataFilters>
    <canonicalLinkDetector
        class="com.norconex.crawler.web.canon.impl.GenericCanonicalLinkDetector">
      <contentTypes>text/html</contentTypes>
    </canonicalLinkDetector>
    <metadataChecksummer
        class="com.norconex.crawler.web.checksum.impl.LastModifiedMetadataChecksummer"
        keep="true"
        toField="myfield"/>
    <robotsMeta
        class="com.norconex.crawler.web.robot.impl.StandardRobotsMetaProvider">
      <headersPrefix>prefix</headersPrefix>
    </robotsMeta>
    <linkExtractors>
      <extractor
          charset="UTF-8"
          class="com.norconex.crawler.web.link.impl.HtmlLinkExtractor"
          commentsEnabled="true"
          ignoreNofollow="false"
          maxURLLength="999">
        <restrictTo>
          <fieldMatcher>document.contentType</fieldMatcher>
          <valueMatcher>text/html</valueMatcher>
        </restrictTo>
        <schemes>https</schemes>
        <tags>
          <tag name="a" attribute="href" />
          <tag name="frame" attribute="src" />
          <tag name="iframe" attribute="src" />
          <tag name="img" attribute="src" />
          <tag name="meta" attribute="http-equiv" />
        </tags>
        <extractBetween ignoreCase="false">
          <start>start1</start><end>end1</end>
        </extractBetween>
        <extractBetween ignoreCase="true">
          <start>start2</start><end>end2</end>
        </extractBetween>
        <noExtractBetween ignoreCase="false">
          <start>nostart1</start><end>noend1</end>
        </noExtractBetween>
        <noExtractBetween ignoreCase="true">
          <start>nostart2</start><end>noend2</end>
        </noExtractBetween>
      </extractor>
      <extractor class="com.norconex.crawler.web.link.impl.HtmlLinkExtractor">
        <tags>
          <tag name="a" attribute="href" />
          <tag name="script" attribute="src" />
        </tags>
      </extractor>
      <extractor class="com.norconex.crawler.web.link.impl.RegexLinkExtractor"
          maxURLLength="1234" charset="iso-8859-1">
        <restrictTo>
          <fieldMatcher>document.contentType</fieldMatcher>
          <valueMatcher method="regex">ct.*</valueMatcher>
        </restrictTo>          
        <restrictTo>
          <fieldMatcher>document.reference</fieldMatcher>
          <valueMatcher method="regex">ref.*</valueMatcher>
        </restrictTo>          
        <linkExtractionPatterns>
          <pattern><match>\[(.*?)\]</match><replace>$1</replace></pattern>
          <pattern><match>http://.*?\.html</match></pattern>
        </linkExtractionPatterns>
      </extractor>
      <extractor class="com.norconex.crawler.web.link.impl.XMLFeedLinkExtractor">
        <restrictTo>
          <fieldMatcher>document.contentType</fieldMatcher>
          <valueMatcher method="regex">.*</valueMatcher>
        </restrictTo>          
        <restrictTo>
          <fieldMatcher>document.reference</fieldMatcher>
          <valueMatcher method="regex">.*</valueMatcher>
        </restrictTo> 
      </extractor>      
    </linkExtractors>
    <documentFilters>
      <filter
          class="com.norconex.crawler.core.filter.impl.ExtensionReferenceFilter"
          ignoreCase="false"
          onMatch="exclude">
        xml,pdf,doc
      </filter>
      <filter class=".GenericReferenceFilter" onMatch="exclude">
        <valueMatcher ignoreCase="true" method="regex">.*example.com.*</valueMatcher>
      </filter>
      <filter class=".GenericMetadataFilter" onMatch="include">
        <fieldMatcher>title</fieldMatcher>
        <valueMatcher ignoreCase="true" method="regex">Blah.*</valueMatcher>
      </filter>
      <filter
          class="com.norconex.crawler.web.filter.impl.SegmentCountURLFilter"
          count="5"
          duplicate="false"
          onMatch="exclude">
        <separator>/</separator>
      </filter>
    </documentFilters>
    <preImportProcessors>
      <processor class="com.norconex.crawler.web.processor.impl.FeaturedImageProcessor">
         <pageContentTypePattern>text/html</pageContentTypePattern>
         <domSelector>dom dom</domSelector>
         <minDimensions>425x312</minDimensions>
         <largest>true</largest>
         <imageCacheSize>1234</imageCacheSize>
         <imageCacheDir>/some/path</imageCacheDir>
         <storage>url, inline</storage>
         <scaleQuality>medium</scaleQuality>
         <scaleDimensions>25</scaleDimensions>
         <scaleStretch>true</scaleStretch>
         <imageFormat>gif</imageFormat>
         <storageDiskDir structure="datetime">/some/other/path</storageDiskDir>
         <storageDiskField>diskField</storageDiskField>
         <storageInlineField>inlineField</storageInlineField>
         <storageUrlField>urlField</storageUrlField>
      </processor>      
    </preImportProcessors>
    <!--v4-importer.xml-->
    <documentChecksummer 
        class="com.norconex.crawler.core.checksum.impl.MD5DocumentChecksummer"
        keep="true" toField="afield">
      <fieldMatcher method="csv">field1,field2</fieldMatcher>
    </documentChecksummer>
    <postImportProcessors>
      <processor />
    </postImportProcessors>
    <postImportLinks keep="true">
      <fieldMatcher method="wildcard">myurls*</fieldMatcher>
    </postImportLinks>
    <spoiledReferenceStrategizer 
        class="com.norconex.crawler.core.spoil.impl.GenericSpoiledReferenceStrategizer"
        fallbackStrategy="DELETE">
      <mapping state="NOT_FOUND" strategy="DELETE" />
      <mapping state="BAD_STATUS" strategy="DELETE" />
      <mapping state="ERROR" strategy="IGNORE" />
    </spoiledReferenceStrategizer>
    <committers>
      <committer class="com.norconex.committer.core.fs.impl.JSONFileCommitter">
        <directory>/tmp/somepath</directory>
      </committer>
    </committers>
  </crawlerDefaults>
  <crawlers>
    <crawler id="myCrawler1">
      <numThreads>2</numThreads>
      <maxDocuments>222</maxDocuments>
      <orphansStrategy>DELETE</orphansStrategy>
    </crawler>
    <crawler id="myCrawler2">
      <numThreads>3</numThreads>
      <maxDocuments>333</maxDocuments>
      <orphansStrategy>IGNORE</orphansStrategy>
    </crawler>
  </crawlers>
</crawlSession>