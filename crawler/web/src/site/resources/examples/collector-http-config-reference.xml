<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE xml>
<!-- 
   Copyright 2010-2023 Norconex Inc.

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<!-- This self-documented configuration file is meant to be used as a reference
     or starting point for a new configuration. 
     It contains most core features offered in this release.  Sometimes
     multiple implementations are available for a given feature. Refer 
     to site documentation for more options and complete description of 
     each features.
     -->
<httpcollector id="My Collector Name">

  <!-- Variables: Optionally define variables in this configuration file
       using the "set" directive, or by using a file of the same name
       but with the extension ".variables" or ".properties".  Refer 
       to site documentation to find out what each extension does.
       Also, one can pass an optional properties file when starting the
       crawler.  Variables are a good way to maintain environment-specific
       values in an independent way.
       -->
  #set($workdir = "./myworkdir")

  <!-- Collector working directory.  This is where files downloaded or created
     as part of crawling activities get stored. Sub-directories will be
     automatically created matching the collector and crawler ids.
     Default is the following (relative to the process current directory):
     -->
  <workDir>${workdir}</workDir>
  
  <!-- One or more optional listeners to be notified on collector
       related events (e.g. starts or finishes).  Class must implement 
       com.norconex.commons.lang.event.IEventListener
       -->
  <eventListeners>
    <listener class="YourClass"/>
  </eventListeners>
  
  <!-- Maximum number of crawlers to run at once. Only useful when you have
       multiple crawlers defined. Default runs all crawlers simultaneously.
       -->
  <maxConcurrentCrawlers>-1</maxConcurrentCrawlers>

  <!-- All crawler configuration options (except for the crawler "id") 
       can be set here as default to prevent repeating them.
       Settings defined here will be inherited by all individual crawlers 
       defined further down, unless overwritten.
       Configuration blocks defined for a specific crawler always takes
       precendence. If you replace a top level crawler tag from the crawler 
       defaults, all the default tag configuration settings will be replaced, 
       no attempt will be made to merge or append.
       -->
  <crawlerDefaults>
    <!-- Set any defaults here. -->
  </crawlerDefaults>

  <!-- Individual crawlers are defined here.  All crawler default
       configuration settings will apply to all crawlers created unless 
       explicitly overwritten in crawler configuration.
       For configuration options where multiple items can be present 
       (e.g. filters), the entire list set in crawler defaults would be
       overwritten.
       -->
  <crawlers>
  
    <!-- Each crawler must have an ID "id" attribute that uniquely 
         identifies it.
         -->    
    <crawler id="My Crawler Name 1">

      <!-- Mandatory starting URL(s) where crawling begins.  If you put more 
           than one URL, they will be processed together.  You can also
           point to one or more URLs files (i.e., seed lists), or
           point to a sitemap.xml. If the list of URLs needs to be dynamically
           built, you can use one or more custom IStartURLsProvider.
           -->    
      <startURLs 
          stayOnDomain="false" 
          includeSubdomains="false" 
          stayOnPort="false" 
          stayOnProtocol="false"
          async="true">
        <url>http://www.example.com</url>
        <url>http://www.sample.com</url>
        <urlsFile>/local/path/to/a/file/full/of/urls.txt</urlsFile>
        <sitemap>http://www.somewhere.com/sitemap.xml</sitemap>
        <provider class="YourClass"/>
      </startURLs>
  
      <!-- Optional URL normalization feature. The class must implement
           com.norconex.crawler.web.url.URLNormalizer, 
           like the following class does.
           -->
      <urlNormalizer class="GenericURLNormalizer">
        <!-- Default normalizations: -->
        <normalizations>
          decodeUnreservedCharacters,
          encodeNonURICharacters,
          lowerCaseSchemeHost,
          removeDefaultPort,
          removeFragment,
          upperCaseEscapeSequence
        </normalizations>
        <replacements>
          <!-- Sample replacement: -->
          <replace>
            <match>&amp;view=print</match>
            <replacement>&amp;view=html</replacement>
          </replace>
        </replacements>
      </urlNormalizer>
  
      <!-- Optional delay resolver defining how polite or aggressive you want
           your crawling to be.  The class must implement 
           com.norconex.crawler.web.delay.DelayResolver.
           The following class is the default implementation 
           (but the schedule sample is not):
           -->
      <delay default="1000" ignoreRobotsCrawlDelay="true"
          class="GenericDelayResolver">
        <schedule dayOfWeek="from Monday to Friday" 
            time="from 8:00 to 16:30">10000</schedule>
      </delay>
  
      <!-- How many threads you want a crawler to use.  Regardless of how many
           thread you have running, the frequency of each URL being invoked
           will remain dictated by the &lt;delay/&gt option above.  Using more
           than one thread is a good idea to ensure the delay is respected
           in case you run into single downloads taking more time than the
           delay specified. Default is 2 threads.
           -->
      <numThreads>2</numThreads>
  
      <!-- How many level deep can the crawler go. I.e, within how many clicks 
           away from the main page (start URL) each page can be to be 
           considered. Beyond the depth specified, pages are rejected.
           The starting URLs all have a zero-depth.  Default is -1 (unlimited)
           -->
      <maxDepth>5</maxDepth>
      
      <!-- Stop crawling after how many successfully processed documents.  
           A successful document is one that is either new or modified, that was 
           not rejected, not deleted, or did not generate any error.  As an
           example, this is a document that will end up in your search engine. 
           Default is -1 (unlimited)
           -->
      <maxDocuments>-1</maxDocuments>

      <!-- Performs a separate HTTP "HEAD" request on every URL to first
           obtain the HTTP response headers without getting the document.
           This can be useful when relying on the obtained metadata to filter
           documents (saving unecessary downloads).  -->
      <fetchHttpHead>false</fetchHttpHead>
  

  
      <!-- Keep downloaded files. Default is false.
           -->
      <keepDownloads>false</keepDownloads>
  
      <!-- Comma-separated list of type of extracted links to keep 
           in metadata. Default is INSCOPE.
           -->
      <keepReferencedLinks>[INSCOPE|OUTSCOPE|MAXDEPTH]</keepReferencedLinks>
  
      <!-- What to do with orphan documents.  Orphans are valid 
           documents, which on subsequent crawls can no longer be reached when 
           running the crawler (e.g. there are no links pointing to that page 
           anymore).  Available options are: 
           IGNORE, DELETE, and PROCESS (default).
           -->
      <orphansStrategy>PROCESS</orphansStrategy>
  
      <!-- One or more fully qualified names of Java exceptions
           that should force a crawler to stop when triggered during the 
           processing of a document. 
           Default is empty (will try to continue upon exceptions).
           -->
      <stopOnExceptions>
        <exception>com.norconex.committer.core.CommitterException</exception>
      </stopOnExceptions>
  
      <!-- Optional event listeners to be notified on various 
           events (e.g. document rejected, document imported, etc). 
           Class must implement 
           com.norconex.commons.lang.event.IEventListener
           -->
      <eventListeners>
        <listener class="YourClass"/>
      </eventListeners>
  
      <!-- Factory class creating a database for storing crawl status and
           other information.  Classes must implement 
           com.norconex.crawler.core.data.store.ICrawlURLDatabaseFactory.  
           Default implementation is the following.
           -->
      <dataStoreEngine class="MVStoreDataStoreEngine" />
  
      <!-- One or more HTTP Fetchers, responsible for making HTTP requests
           and fetching associated content. Fetchers are defined in execution 
           order. If the first fails or does not support a given URL, the next 
           fetcher will try to fetch it, and so on. Classes must implement 
           com.norconex.crawler.web.fetch.HttpFetcher
           Default implementation is the following.
           -->
      <httpFetchers>
        <fetcher class="GenericHttpFetcher" maxRetries="0" retryDelay="0"/>
      </httpFetchers>
  
      <!-- Optionally filter URL BEFORE any download. Classes must implement 
           com.norconex.crawler.core.filter.IReferenceFilter, 
           like the following examples.
           -->
      <referenceFilters>
        <filter class="ExtensionReferenceFilter" onMatch="exclude">
          jpg,gif,png,ico,css,js</filter>
        <filter class="RegexReferenceFilter">https://www.example.com/.*</filter>
      </referenceFilters>
  
      <!-- Filter BEFORE download with RobotsTxt rules. Classes must
           implement *.robot.IRobotsTxtProvider.  Default implementation
           is the following.
           -->
      <robotsTxt ignore="false" class="StandardRobotsTxtProvider"/>
      
      <!-- Loads sitemap.xml URLs and adds adds them to URLs to process.
           Default implementation is the following.
           -->
      <sitemapResolver ignore="false" lenient="false" class="GenericSitemapResolver">
        <path>/blogs/sitemap.xml</path>
      </sitemapResolver>
      
      <!-- Provides the target URL to use when a redirect is encountered.
           Classes must implement *.redirect.IRedirectURLProvider.
           Default implementation is the following.
           -->
      <redirectURLProvider class="GenericRedirectURLProvider" fallbackCharset="" />
      
      <!-- Indicates if a target URL is ready for recrawl or not.
           Default implementation is the following.
           -->
      <recrawlableResolver class="GenericRecrawlableResolver" />
      
      <!-- Optionally filter AFTER download of HTTP headers.  Classes must 
           implement com.norconex.crawler.core.filter.IMetadataFilter.  
           -->
      <metadataFilters>
        <filter class="RegexMetadataFilter" 
                onMatch="exclude"
                caseSensitive="false"
                field="Content-Type">.*css.*</filter>
      </metadataFilters>        
  
      <!-- Detect canonical links. Classes must implement
           com.norconex.crawler.web.canon.CanonicalLinkDetector.
           Default implementation is the following.
           -->
      <canonicalLinkDetector ignore="false" class="GenericCanonicalLinkDetector">
        <contentTypes>
            text/html, application/xhtml+xml, vnd.wap.xhtml+xml, x-asp
        </contentTypes>
      </canonicalLinkDetector>
  
      <!-- Generates a checksum value from document headers to find out if 
           a document has changed. Class must implement
           com.norconex.crawler.core.checksum.IMetadataChecksummer.  
           Default implementation is the following. 
           -->
      <metadataChecksummer class="LastModifiedMetadataChecksummer" />
  
      <!-- Establish whether to follow a page URLs or to index a given page
           based on in-page meta tag robot information. Classes must implement 
           com.norconex.crawler.web.robot.RobotsMetaProvider.  
           Default implementation is the following.
           -->
      <robotsMeta ignore="false" class="StandardRobotsMetaProvider" />
  
      <!-- Extract links from a document.  Classes must implement
           com.norconex.crawler.web.link.LinkExtractor. 
           Default implementation is the following.
           -->
      <linkExtractors>
        <extractor class="HtmlLinkExtractor"  maxURLLength="2048" 
            ignoreNofollow="false" commentsEnabled="false">
          <contentTypes>
            text/html, application/xhtml+xml, vnd.wap.xhtml+xml, x-asp
          </contentTypes>
          <tags>
            <tag name="a" attribute="href" />
            <tag name="frame" attribute="src" />
            <tag name="iframe" attribute="src" />
            <tag name="img" attribute="src" />
            <tag name="meta" attribute="http-equiv" />
          </tags>
        </extractor>
      </linkExtractors>
  
      <!-- Optionally filters a document. Classes must implement 
           com.norconex.crawler.core.filter.IDocumentFilter
           -->
      <documentFilters>
        <filter class="YourClass" />
      </documentFilters>
  
      <!-- Optionally process a document BEFORE importing it. Classes must
           implement com.norconex.crawler.web.doc.IHttpDocumentProcessor.
           -->
      <preImportProcessors>
        <processor class="YourClass"></processor>
      </preImportProcessors>
        
      <!-- Import a document.  This step calls the Importer module.  The
           importer is a different module with its own set of XML configuration
           options.  Please refer to Importer for complete documentation.
           Below gives you an overview of the main importer tags.
           -->
      <importer>
        <preParseHandlers>
          <handler class="..."/>
          ...
        </preParseHandlers>
        <documentParserFactory class="..." />
        <postParseHandlers>
          <handler class="..." />
          ...
        </postParseHandlers>
        <responseProcessors>
          <responseProcessor class="YourClass" />
        </responseProcessors>
      </importer>           
  
  
      <!-- Create a checksum out of a document to figure out if a document
           has changed, AFTER it has been imported. Class must implement 
           com.norconex.crawler.core.checksum.IDocumentChecksummer.
           Default implementation is the following.
           -->
      <documentChecksummer class="MD5DocumentChecksummer" />
  
      <!-- Optionally process a document AFTER importing it. Classes must
           implement com.norconex.crawler.web.doc.IHttpDocumentProcessor.
           -->
      <postImportProcessors>
        <processor class="YourClass"></processor>
      </postImportProcessors>
        
      <!-- Optionally define one or more post-import fields containing URLs
           to be considered for crawling. The field will be deleted
           once read, unless "keep" is "true".
           -->
      <postImportLinks keep="[false|true]">
        <fieldMatcher></fieldMatcher>
      </postImportLinks>
        
        
      <!-- Decide what to do with references that have turned bad.
           Class must implement 
           com.norconex.crawler.core.spoil.ISpoiledReferenceStrategizer.
           Default implementation is the following.
           -->
      <spoiledReferenceStrategizer class="GenericSpoiledReferenceStrategizer"
              fallbackStrategy="DELETE">
        <mapping state="NOT_FOUND"  strategy="DELETE" />
        <mapping state="BAD_STATUS" strategy="GRACE_ONCE" />
        <mapping state="ERROR"      strategy="GRACE_ONCE" />
      </spoiledReferenceStrategizer>
        
      <!-- Commits a document to a data source of your choice.
           This step calls the Committer module.  The
           committer is a different module with its own set of XML configuration
           options.  Please refer to committer for complete documentation.
           Below is an example using the XMLFileCommitter.
           -->
      <committers>
        <committer class="XMLFileCommitter">
        </committer>
      </committers>

    </crawler>

    <crawler id="My Crawler Name 2">
      <!-- You can define any number of crawlers. -->
    </crawler>

  </crawlers>

</httpcollector>