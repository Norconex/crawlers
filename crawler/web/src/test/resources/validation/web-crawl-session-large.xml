<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE xml>
<!-- 
   Copyright 2017-2024 Norconex Inc.

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<!-- This is a config file with as many configuration options being set
     as possible, to test validation as much we can.
     -->
<crawler id="Test Web Crawler">
  <workDir>/workdir</workDir>
  
  <eventListeners>
    <listener class="UrlStatusCrawlerEventListener">
      <statusCodes>404</statusCodes>
      <outputDir>/tmp/path</outputDir>
      <fileNamePrefix>broken-links</fileNamePrefix>
    </listener>
  </eventListeners>
  
  <startReferences>
    <ref>http://www.example.com</ref>
    <ref>http://www.sample.com</ref>
  </startReferences>
  <startReferencesFiles>
    <file>/local/path/to/a/file/full/of/urls.txt</file>
  </startReferencesFiles> 
  <startReferencesSitemaps>
    <sitemap>http://www.somewhere.com/sitemap.xml</sitemap>
  </startReferencesSitemaps>
  <startReferencesProviders>
    <provider class="MockStartURLsProvider"/>
  </startReferencesProviders>
  <urlScopeResolver>
    <class>GenericUrlScopeResolver</class>
    <stayOnDomain>true</stayOnDomain> 
    <includeSubdomains>true</includeSubdomains>
    <stayOnPort>true</stayOnPort> 
    <stayOnProtocol>true</stayOnProtocol>
  </urlScopeResolver>
  <startReferencesAsync>true</startReferencesAsync>

  <keepReferencedLinks>INSCOPE, OUTSCOPE, MAXDEPTH</keepReferencedLinks>
  <maxDepth>99</maxDepth>
  <numThreads>1</numThreads>
  <maxDocuments>111</maxDocuments>
  <orphansStrategy>PROCESS</orphansStrategy>
  <metadataFetchSupport>OPTIONAL</metadataFetchSupport>
  <documentFetchSupport>DISABLED</documentFetchSupport>

  <urlNormalizer class="GenericUrlNormalizer">
    <normalizations>
      lowerCaseSchemeHost, upperCaseEscapeSequence, 
      decodeUnreservedCharacters, removeDefaultPort 
    </normalizations>
    <replacements>
      <replace><match>A</match><value>B</value></replace>
      <replace><match>C</match></replace>
    </replacements>
  </urlNormalizer>

  <delayResolver class="GenericDelayResolver">
    <defaultDelay>1s</defaultDelay>
    <ignoreRobotsCrawlDelay>true</ignoreRobotsCrawlDelay>
    <scope>crawler</scope>
    <schedules>
      <schedule>
        <dayOfWeekRange>
          <start>MON</start><end>FRI</end>
        </dayOfWeekRange>
        <dayOfMonthRange>
          <start>1</start><end>10</end>
        </dayOfMonthRange>
        <timeRange>
          <start>8:00</start><end>16:30</end>
        </timeRange>
        <delay>10s</delay>
      </schedule>
      <schedule>
        <dayOfWeekRange start="SAT" end="SUN"/>
        <dayOfMonthRange start="11" end="28"/>
        <timeRange start="6:00" end="7:30"/>
        <delay>20000</delay>
      </schedule>
    </schedules>
  </delayResolver>    

  <dataStoreEngine class="com.norconex.crawler.core.store.impl.mvstore.MVStoreDataStoreEngine">
    <pageSplitSize>1000</pageSplitSize>
    <compress>1</compress>
    <cacheConcurrency>0</cacheConcurrency>
    <cacheSize>1000</cacheSize>
    <autoCompactFillRate>100</autoCompactFillRate>
    <autoCommitBufferSize>100</autoCommitBufferSize>
    <autoCommitDelay>100</autoCommitDelay>    
  </dataStoreEngine>

  <fetchersMaxRetries>5</fetchersMaxRetries>
  <fetchersRetryDelay>10 seconds</fetchersRetryDelay>
  <fetchers>
    <fetcher class="com.norconex.crawler.web.fetch.impl.GenericHttpFetcher">
      <userAgent>Here we crawl!</userAgent>
      <validStatusCodes>200,201</validStatusCodes>
      <notFoundStatusCodes>404,401</notFoundStatusCodes>
      <forceContentTypeDetection>true</forceContentTypeDetection>
      <forceCharsetDetection>true</forceCharsetDetection>
      <headersPrefix>myprefix</headersPrefix>
      <forceContentTypeDetection>true</forceContentTypeDetection>
      <forceCharsetDetection>true</forceCharsetDetection>
      <redirectUrlProvider fallbackCharset="UTF-8"/>
      <cookieSpec>ignore</cookieSpec>
      <authentication>
        <formParams>
          <param1>value1</param1>
          <param2>value1</param2>
        <!-- 
             TODO: shall we favor this so special characters are allowed
             as keys?
           
          <param name="param1">value1</param>
          <param name="param2">value2</param>
        -->
        </formParams>
        <method>digest</method>
        <credentials>
          <username>user</username>
          <password>pass</password>
          <passwordKey>
            <value>/path/to/my.key</value>
            <source>file</source>
          </passwordKey>              
        </credentials>
        <formUsernameField>userfield</formUsernameField>
        <formPasswordField>pwdfield</formPasswordField>
        <formSelector>#myform</formSelector>
        <formCharset>UTF-8</formCharset>
        <url>authURL</url>
        <host>
          <name>host</name>
          <port>9</port>
        </host>
        <realm>authRealm</realm>
        <workstation>authWorkstation</workstation>
        <domain>authDomain</domain>
        <preemptive>true</preemptive>
      </authentication>
      <connectionTimeout>1</connectionTimeout>
      <socketTimeout>2 minutes</socketTimeout>
      <connectionRequestTimeout>3 min 30s</connectionRequestTimeout>
      <expectContinueEnabled>true</expectContinueEnabled>
      <maxRedirects>4</maxRedirects>
      <localAddress>address</localAddress>
      <maxConnections>5</maxConnections>
      <maxConnectionsPerRoute>6</maxConnectionsPerRoute>
      <maxConnectionIdleTime>7</maxConnectionIdleTime>
      <maxConnectionInactiveTime>8</maxConnectionInactiveTime>
      <trustAllSSLCertificates>true</trustAllSSLCertificates>
      <sniDisabled>true</sniDisabled>
      <sslProtocols>item1,item2</sslProtocols>
      <proxySettings>
        <host>
          <name>host</name>
          <port>9</port>
        </host>
        <realm>realm</realm>
        <scheme>scheme</scheme>
        <credentials>
          <username>username</username>
          <password>pwd</password>
          <passwordKey>
            <value>/path/to/my.key</value>
            <source>file</source>
          </passwordKey>              
        </credentials>
      </proxySettings>
      <ifModifiedSinceDisabled>true</ifModifiedSinceDisabled>
      <requestHeaders>
        <head1></head1>
        <head2>value2</head2>
      </requestHeaders>
    </fetcher>
  </fetchers>    
    
  <referenceFilters>
    <filter class="ExtensionReferenceFilter">
      <onMatch>exclude</onMatch>
      <ignoreCase>false</ignoreCase>
      <extensions>
       <extensions>xml</extensions>
       <extensions>pdf</extensions>
       <extensions>doc</extensions>
      </extensions>
    </filter>

   <filter class="GenericReferenceFilter">
     <onMatch>exclude</onMatch>
     <valueMatcher ignoreCase="false" method="regex" pattern=".*example.com.*"/>
   </filter>
   <filter class="SegmentCountUrlFilter"
            onMatch="exclude" count="5" duplicate="false">
      <separator>/</separator>
    </filter>
  </referenceFilters>    


  <robotsTxtProvider
     class="StandardRobotsTxtProvider"/>
  <sitemapResolver lenient="true" class="GenericSitemapResolver">
  </sitemapResolver>
  <sitemapLocator class="GenericSitemapLocator" robotsTxtSitemapDisabled="true">
    <paths>
      <path>/path1/</path>
      <path>/path2/</path>
    </paths>
  </sitemapLocator>

  <recrawlableResolver class="GenericRecrawlableResolver"
      sitemapSupport="last" >
    <minFrequencies>
      <minFrequency applyTo="reference" value="always" >
        <matcher method="regex" pattern=".*\.pdf"/>
      </minFrequency>
      <minFrequency applyTo="contentType" value="3000" >
        <matcher pattern="text/html"/>
      </minFrequency>
    </minFrequencies>
  </recrawlableResolver>

  <metadataFilters>
    <filter class="ExtensionReferenceFilter"
        onMatch="exclude" ignoreCase="false">
      <extensions>
        <extension>xml</extension>
        <extension>pdf</extension>
        <extension>doc</extension>
      </extensions>
    </filter>
    <filter class="GenericReferenceFilter" onMatch="exclude">
      <valueMatcher ignoreCase="true" method="regex" pattern=".*example.com.*"/>
    </filter>
    <filter class="GenericMetadataFilter" onMatch="include">
      <valueMatcher ignoreCase="true" method="regex" pattern="Blah.*"/>
      <fieldMatcher>title</fieldMatcher>
    </filter>
    <filter class="SegmentCountUrlFilter"
            onMatch="exclude" count="5" duplicate="false">
      <separator>/</separator>
    </filter>
  </metadataFilters>

  <canonicalLinkDetector class="GenericCanonicalLinkDetector">
    <contentTypes>
      <contentType>text/html</contentType>
   </contentTypes>
  </canonicalLinkDetector>
  <metadataChecksummer 
      class="LastModifiedMetadataChecksummer"
      keep="true" toField="myfield" />

  <robotsMetaProvider class="StandardRobotsMetaProvider">
     <headersPrefix>prefix</headersPrefix>
  </robotsMetaProvider>

  <linkExtractors>
    <extractor class="HtmlLinkExtractor"
        maxURLLength="999" ignoreNofollow="false" 
        commentsEnabled="true" charset="UTF-8" >
      <contentTypeMatcher pattern="text/html"/>
      <schemes>https</schemes>
      <tagAttribs>
        <tagAttrib name="a" attribute="href" />
        <tagAttrib name="frame" attribute="src" />
        <tagAttrib name="iframe" attribute="src" />
        <tagAttrib name="img" attribute="src" />
        <tagAttrib name="meta" attribute="http-equiv" />
      </tagAttribs>
      <extractBetweens>
        <extractBetween ignoreCase="false">
          <start>start1</start><end>end1</end>
        </extractBetween>
        <extractBetween ignoreCase="true">
          <start>start2</start><end>end2</end>
        </extractBetween>
        <noExtractBetween ignoreCase="false">
          <start>nostart1</start><end>noend1</end>
        </noExtractBetween>
        <noExtractBetween ignoreCase="true">
          <start>nostart2</start><end>noend2</end>
        </noExtractBetween>
      </extractBetweens>
    </extractor>
    <extractor class="HtmlLinkExtractor">
      <tagAttribs>
        <tagAttrib name="a" attribute="href" />
        <tagAttrib name="script" attribute="src" />
      </tagAttribs>
    </extractor>
    <extractor class="RegexLinkExtractor"
        maxUrlLength="1234" charset="iso-8859-1">
      <restrictions>
        <restriction>
          <fieldMatcher>document.contentType</fieldMatcher>
          <valueMatcher method="regex" pattern="ct.*"/>
        </restriction>
        <restriction>
        <fieldMatcher>document.reference</fieldMatcher>
        <valueMatcher method="regex" pattern="ref.*"/>
        </restriction>
      </restrictions>
      <patterns>
        <pattern><match>\[(.*?)\]</match><replace>$1</replace></pattern>
        <pattern><match>http://.*?\.html</match></pattern>
      </patterns>
    </extractor>
    <extractor class="XmlFeedLinkExtractor">
      <contentTypeMatcher pattern=".*"/>
    </extractor>      
  </linkExtractors>

  <documentFilters>
    <filter class="ExtensionReferenceFilter"
        onMatch="exclude" ignoreCase="false">
      <extensions>
        <extension>xml</extension>
        <extension>pdf</extension>
        <extension>doc</extension>
      </extensions>
    </filter>
    <filter class="GenericReferenceFilter" onMatch="exclude">
      <valueMatcher ignoreCase="true" method="regex" pattern=".*example.com.*"/>
    </filter>
    <filter class="GenericMetadataFilter" onMatch="include">
      <fieldMatcher>title</fieldMatcher>
      <valueMatcher ignoreCase="true" method="regex" pattern="Blah.*"/>
    </filter>
    <filter class="SegmentCountUrlFilter"
            onMatch="exclude" count="5" duplicate="false">
      <separator>/</separator>
    </filter>
  </documentFilters>

  <preImportConsumers>
    <consumer class="FeaturedImageProcessor">
       <pageContentTypePattern>text/html</pageContentTypePattern>
       <domSelector>dom dom</domSelector>
       <minDimensions>425x312</minDimensions>
       <largest>true</largest>
       <imageCacheSize>1234</imageCacheSize>
       <imageCacheDir>/some/path</imageCacheDir>
       <storage>url, inline</storage>
       <scaleQuality>medium</scaleQuality>
       <scaleDimensions>25</scaleDimensions>
       <scaleStretch>true</scaleStretch>
       <imageFormat>gif</imageFormat>
       <storageDiskDir>/some/other/path</storageDiskDir>
       <storageDiskStructure>datetime</storageDiskStructure>
       <storageDiskField>diskField</storageDiskField>
       <storageInlineField>inlineField</storageInlineField>
       <storageUrlField>urlField</storageUrlField>
    </consumer>      
  </preImportConsumers>

  <!-- Importer is purposely slim since the full config is tested in
       Importer project. -->
  <importer>
    <handlers>
      <handler class="com.norconex.importer.handler.transformer.impl.ReplaceTransformer">
        <operations>
          <operation>
            <valueMatcher pattern="A"/><toValue>B</toValue>
          </operation>
        </operations>
      </handler>  
      <handler class="com.norconex.importer.handler.transformer.impl.ReplaceTransformer">
        <operations>
          <operation>
            <valueMatcher pattern="C"/><toValue>D</toValue>
          </operation>
        </operations>
      </handler>  
    </handlers>
  </importer>

  <documentChecksummer 
      class="MD5DocumentChecksummer"
      keep="true" toField="afield">
    <fieldMatcher method="csv" pattern="field1,field2"/>
  </documentChecksummer>
  <postImportConsumers>
    <consumer />
  </postImportConsumers>
  <postImportLinksKeep>true</postImportLinksKeep>
  <postImportLinks method="wildcard" pattern="myurls*"/>
  <spoiledReferenceStrategizer 
      class="GenericSpoiledReferenceStrategizer"
      fallbackStrategy="DELETE">
    <mappings>
      <mapping state="NOT_FOUND" strategy="DELETE" />
      <mapping state="BAD_STATUS" strategy="DELETE" />
      <mapping state="ERROR" strategy="IGNORE" />
    </mappings>
  </spoiledReferenceStrategizer>

  <committers>
    <committer class="JSONFileCommitter">
      <directory>/tmp/somepath</directory>
    </committer>
  </committers>

</crawler>