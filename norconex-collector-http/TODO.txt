TODO:
==============

The following are either things to be done, or ideas to consider:

- Create a class holding constants for frequent XML config attributes. 

- Consider porting the HttpImporterPipelineContext#httpHeadSuccessful update
  to continue processing a doc if HTTP HEAD fails.
  https://github.com/Norconex/collector-http/issues/655

- Enforce naming convention where possible (e.g. disabled vs ignored).

- GenericHttpFetcher: rename "headers" to "requestHeaders", rename 
  "headersPrefix" to "responseHeadersPrefix" and add:
  responseHeadersDiscard.  No need to add responseHeadersOnSet since
  http headers are the first thing obtained (or do it since we could get
  both HEAD and GET headers).

- Document that the data store can be used for anything now, not just crawl data.

- Document all migration changes.

- CrawlReference is casted to HttpCrawlReference in many places.
  Use generic in Collector Core to prevent casting?

- Move changes.xml to project root folder to make it obvious?

- Allow storing document/metadata checksummers as null (instead or same as 
  disabled).

- <referenceFilters> should be urlFilters to avoid conflict with
  Importer equivalent?

- Eliminate CachedInputStream.dispose() in favor of close() to be consistent?

- Add a reference filter that matches URLs with the same segment repeated
  Nth consecutive times.

- Add ability to export crawlstore in neutral format to switch from one to 
  another.

- Remove MultiCommitter in favor of supporting multiple committers natively.

- Do lib dups check on startup.

- Fix javadoc (including links) all over.

- Use Norconex parent project for all collector projects

- Switch to short semantic versioning on Github, and get rid of "develop"
  branch concept (where master is trunk).

- Have ability in XML to only create object (IXMLConfigurable) if any child
  elements are defined (for optional config blocks). 

- Add xdoclet or equivalent that could be extracted from java doc to build
  online doc automatically/dynamically.

- Add ability to disable storing HTTP headers as metadata fields.

- Add update.sh scripts to collectors (doing the same as install.sh script)

- Provide generic way to access JSON APIs (using standard auth methods, 
  ability to provide how to access next pages, etc).

- Provide support for new popular auth methods.

- Offer to overwrite httpFetcher-specified User-Agent directly on 
  classes that needs it, like the Robots.txt related class.

- Try version 2.12.0 of apache xerces that came up in June 2018.

- Where appropriate, remove the many places where we pass an collector or 
  http config in favor of those classes being able to get it on startup or 
  statically.

- Remove stop() methods in a few classes and listen for crawler/collector
  shutdown instead. 

- Have logging show thread name and make it crawler id (and stop adding 
  it in logging statements found in many places).

- Have a generic page describing password encryption and point all javadoc to
  it (could be the javadoc page from Nx Commons Lang.

- ChromeDriver:
  1. Download latest chrome driver:
     https://sites.google.com/a/chromium.org/chromedriver/downloads
  2. Make sure you have chrome installed to default location, or 
     specify custom location as per: 
     https://sites.google.com/a/chromium.org/chromedriver/capabilities#TOC-Using-a-Chrome-executable-in-a-non-standard-location
  3. Add ChromeDriver to Windows PATH or set system property in Java. E.g.:
       System.setProperty("webdriver.chrome.driver", "/path/to/chromedriver");

- MAYBE: Upon encountering specific warnings, make command line interfactive
  by default (unless headless detected) and provide option to turn off.

- Optionally keep the URL trail or at least start URL for document:
  https://github.com/Norconex/collector-http/issues/514

- Form authentication: add ability to first parse a login page, look for 
  login form, and pass all its input field to the login request.

- Have ability to download and install committers from command line?

- Have ability to do POST instead of GET for matching URLs.

- Because the QueuePipeline can be invoked from anywhere, make the 
  "inscope" check part of it instead of always forcing to check first.

- Have an option that sends committer deletion requests at every rejection
  (once per session per URLs).

- Support custom arguments to the PhantomJS script file.
  https://github.com/Norconex/collector-http/issues/505

- Support HTTP v2.  Example site: https://www.eetimes.com/rss_simple.asp

- Add external site depth?

- Create parent project to set managed dependencies?

- Use safe filename methods for saving sitemap data (otherwise fails when 
  containing spaces or others).

- Add one of these as JVM arg: 
      -XX:ExitOnOutOfMemoryError
      -XX:CrashOnOutOfMemoryError 

- Add to FAQ: how to extract few different HTML tag scenarios and maybe 
  add a tagger that helps with that.

- Maybe rename REJECTED_UNMODIFIED|PREMATURE to IGNORED_* ? 

- Have a new <inScopeStrategy class="..."> with the default implementation
  being [Generic]URLCrawlScopeStrategy, taking the stayOn* and implementing 
  an IURLCrawlScope interface.

- Investigate why defining tags (e.g. <httpClientFactory> under the collector
  root tag (as opposed to under crawler tag) does not produce validation error.

- Image cache gives issues with multiple crawlers using the same path, fix this.

- Make it possible to merge two or more docs.

- Maybe add "stayOnDomainMaxSub" for how many subdomain level to support?
  (default would be zero).

- Make default directory for logs and progress the workdir.

- Fix redirects used to authenticate or create session but otherwise return
  to original URL which then can be crawled.

- Change sample config to have just "$workdir" for logs and progress dirs.

- Change sample config to have crawlerDefaults empty instead.

- Remove <dependencyManagement> which is mainly valuable when used with
  parent projects (which we are not here).  Use regular <dependencies instead.
  Do this for all projects.

- Use Norconex Commons Lang Proxy class wherever proxies are used.

- Use Apache HttpClient-Win for Windows Auth support like done with
  Azure Search Committer.

- Have the config reference.xml put all settings in a crawler instead
  of crawling defaults (FS Collector as well).

- Ship with importer.[sh|bat] files (applies to FS Collector as well).

- Have a way to store all data for easy recrawl.

- Reorganize Basic+Execution tests to combine servlet test cases with their 
  test classes.

- On GenericLinkExtractor, offer a flag to convert \ to /.  Cannot be done
  at normalization time since relative URLs will not be converted to full
  URLs properly without that.

- Consider deprecating TikaLinkExctractor, or at least update it to support
  relative base hrefs (share some code with GenericLinkExtractor).

- Offer a separate process to run on the crawl store and produce some kind of 
  flat file with referring URL counts so people can decide to use this
  to create some form of relevancy in their search engine (e.g. using 
  inboud links as a popularity indicator).

- Have a global settings for making Properties (HttpMetadata, 
  ImporterMetadata, etc) case sensitive or not.

- To consider: when re-processing orphans, check the depth in case the 
  max depth changed.

- Offer a flag to merge frame/iframe elements on HTML pages as opposed to
  crawl them as separate documents.

- Add a command-line flag that saves the constructed config back to disk 
  for easy sharing (XML-write).

- Carry a flag that tells if we are running a full crawl, or an incremental one.
  Some operations may be relevant only on incremental runs.

- Add total execution time, periodic time elapsed and estimated remaining.

- Store somewhere in metadata (and maybe carry in code?) whether a 
  document is new or modified.

- When issuing a STOP, add an option to specify which crawler to stop,
  and let others run.

- Add a startup flag that will generate a batch/sh script for the user which 
  abstracts the call to the config file 
  (e.g., abcCollector.sh start|stop|resume)

- Ability to crawl up to a given size.  Absolute number or percentage of 
  disk capacity? Shall we tie that to checking for remaining space? 
  We could issue warnings and/or stop when threshold is reached to 
  prevent crashing due to lack of space.

- Introduce "Add-ons" like social media add-ons to crawl social media sites.

- Have an interface for how to optionally store downloaded files
  (i.e., location, directory structure, file naming).  This could allow
  usage of the collector to clone a site.  Should the DocumentFetcher do it 
  instead?

- Have a crawler event listener that generated a tree-like graph of all URLs
  (i.e. a kind of sitemap).
  
- To consider: Interface for how to save documents whey they are kept.
  Same with default committer queue location.
  File system is used for both now, but could be others like MongoDB?
  Same as previous item?   

- Add a command prompt action to flush the committer queue. This can be useful
  if the collector crashed for some reason, while there were files left in
  the committer queue.  Shall this be done on an AbstractCommitter class
  so all future collectors automatically has that?

- Add support for having different HTTPContext for each call and/or each
  sites.
  
- Provide some duplicate content detection mechanism.  Maybe using existing 
  checksums, or create a distinction between a "modified" checksum (to find out
  if a document was modified) and a "duplicate" checksum (to find out if 
  documents are identical).

- Add the ability to control how many successive crawls it has to go through
  before deleting a document because it was not found (404).

- Have configurable the level of verbosity desired when logging exceptions.
  The options could be:
     - type: none|all|first|last
     - stacktrace: false|true

- Rotate/break log files when too big.

- If a URL filter rule was changed and a document is now rejected (never 
  processed), it will not be deleted (since it did not get a 404/NOT_FOUND).
  Maybe check if rejected URL in URLProcessor are in cache and send deletion 
  if so.

- Integrate with distributed computing frameworks such as Hadoop.

- Test that IPV6 is supported (as domain names).

- Offer option to have crawling rate adjust itself if a site is slow
  (in case it is the crawler hammering it).  Probably a change or new delay
  implementation... this means total download time (both for HEAD and GET) 
  should be added as document properties (not a bad idea to do regardless).

- Add option to skip certain http response code (like 500).  Those docs 
  that should not be added nor deleted because they are in a temporary bad 
  state.  Is it really useful?

- Add a GUI application to help manage collectors and report useful info.

- Deal with <a rel="noreferrer" ... ??

MAYBE:
=======
- Start offering per-site options?  Like crawl interval and more?
  (can be achieve with defining different crawlers now).
